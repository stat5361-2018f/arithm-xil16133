---
title: "Use the Monte Carlo method to approximate $N(0,1)$"
# subtitle: "possible subtitle goes here"
author:
  - Xiaokang Liu
date: "`r format(Sys.time(), '%d %B %Y')`"
documentclass: article
papersize: letter
fontsize: 11pt
output:
  bookdown::pdf_document2
abstract: |
    This report aims to record a small experiment to use the Monte Carlo method to approximate the cumulative distribution function of the standard normal distribution. The results will be displayed in tables and graphs.
---


```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
## some utility functions, see the source code for details
#source("utils_template.R")

## specify the packages needed
#pkgs <- c("DT", "leaflet", "splines2", "webshot")
#need.packages(pkgs)

## external data can be read in by regular functions,
## such as read.table or load

## for latex and html output
isHtml <- knitr::is_html_output()
isLatex <- knitr::is_latex_output()
latex <- ifelse(isLatex, '\\LaTeX\\', 'LaTeX')

## specify global chunk options
knitr::opts_chunk$set(fig.width = 5, fig.height = 4, dpi = 300,
                      out.width = "90%", fig.align = "center")

```


# Introduction {#sec:intro}
Consider approximation of the distribution function of $N(0,1)$,
\begin{align}
    \Psi(t)=\int_{-\infty}^t \frac{1}{\sqrt{2\pi}}e^{-y^2/2}dy,
\end{align}
by 
\begin{align}
    \widehat \Psi(t)=\frac{1}{n}\sum_{i=1}^n I(X_i \leq t),
\end{align}
where $X_i$'s are i.i.d. $N(0,1)$ variables. Experiments with the approximation at $n \in \{10^2, 10^3, 10^4\}$ at $t \in \{0,0.67,0.84,1.28,1.65,2.32,2.58,3.09,3.72 \}$ will be displayed by a table. The experiment will be repeated for 100 times. The bias at all $t$
will be showed in some boxplots. 


# Implementation and Results 
```{r}
n <- c(100, 1000, 10000)
t <- c(0,0.67,0.84,1.28,1.65,2.32,2.58,3.09,3.72)
results <- array(dim = c(100, 3, 9))
for (i in 1:100){
  for (j in 1:3){
    for (k in 1:9){
      results[i,j,k] <- sum(rnorm(n[j])<=t[k])/n[j]
    }
  }
}
# summarize the results from 100 repeatations
sum.results <- matrix(nrow = 4, ncol = 9)
sum.results[1,] <- pnorm(t)
sum.results[2,] <- apply(results[,1,], 2, mean)
sum.results[3,] <- apply(results[,2,], 2, mean)
sum.results[4,] <- apply(results[,3,], 2, mean)

# table
colnames(sum.results) <- c("0.0","0.67","0.84","1.28","1.65","2.32","2.58",
                           "3.09","3.72")
rownames(sum.results) <- c("true","n=100","n=1000","n=10000")
knitr::kable(sum.results[,c(1:5)], booktabs = TRUE,
             caption = 'Summary of the experiment(part 1)')
knitr::kable(sum.results[,-c(1:5)], booktabs = TRUE,
             caption = 'Summary of the experiment(part 2)')

```

The above two tables including the results averaged from 100 repetations for each situation. By comparing the results of the 2, 3 and 4 row with the 1st row, we can find that larger the sample size, smaller the difference between the approximated probability and the true probability.







