---
title: "Use the Monte Carlo method to approximate the distribution of $N(0,1)$"
# subtitle: "possible subtitle goes here"
author:
  - Xiaokang Liu
date: "`r format(Sys.time(), '%d %B %Y')`"
documentclass: article
papersize: letter
fontsize: 11pt
output:
  bookdown::pdf_document2
abstract: |
    This report includes a small experiment to use the Monte Carlo method to approximate the cumulative distribution function of the standard normal distribution. The results will be displayed in tables and graphs.
---


```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
## some utility functions, see the source code for details
source("utils_template.R")

## specify the packages needed
pkgs <- "ggplot2"
need.packages(pkgs)

## external data can be read in by regular functions,
## such as read.table or load

## for latex and html output
isHtml <- knitr::is_html_output()
isLatex <- knitr::is_latex_output()
latex <- ifelse(isLatex, '\\LaTeX\\', 'LaTeX')

## specify global chunk options
knitr::opts_chunk$set(fig.width = 5, fig.height = 4, dpi = 300,
                      out.width = "90%", fig.align = "center")

```


# Introduction {#sec:intro}
Consider approximation of the distribution function of $N(0,1)$,
\begin{align}
    \Psi(t)=\int_{-\infty}^t \frac{1}{\sqrt{2\pi}}e^{-y^2/2}dy,
\end{align}
by 
\begin{align}
    \widehat \Psi(t)=\frac{1}{n}\sum_{i=1}^n I(X_i \leq t),
\end{align}
where $X_i$'s are i.i.d. $N(0,1)$ variables. Experiments with the approximation at $n \in \{10^2, 10^3, 10^4\}$ at $t \in \{0,0.67,0.84,1.28,1.65,2.32,2.58,3.09,3.72 \}$ will be displayed by a table. The experiment will be repeated for 100 times. The bias at all $t$
will be illustrated by boxplots. 


# Implementation and Results 
## R codes for conducting experiments
```{r}
n <- c(100, 1000, 10000)
t <- c(0,0.67,0.84,1.28,1.65,2.32,2.58,3.09,3.72)
results <- array(dim = c(100, 3, 9))
for (i in 1:100){
  for (j in 1:3){
    for (k in 1:9){
      results[i,j,k] <- sum(rnorm(n[j])<=t[k])/n[j]
    }
  }
}

# summarize the results from 100 repetitions
sum.results <- matrix(nrow = 4, ncol = 9)
sum.results[1,] <- pnorm(t)
sum.results[2,] <- apply(results[,1,], 2, mean)
sum.results[3,] <- apply(results[,2,], 2, mean)
sum.results[4,] <- apply(results[,3,], 2, mean)
colnames(sum.results) <- c("0.0","0.67","0.84","1.28","1.65","2.32","2.58",
                           "3.09","3.72")
rownames(sum.results) <- c("true","n=100","n=1000","n=10000")
```

## Results 
### Tables of mean estimation values

The following two tables including the results averaged from 100 repetitions for each situation. By comparing the results of the 2nd, 3rd and 4th row with the 1st row, we can find that larger the sample size, smaller the difference between the approximated probability and the true probability.

```{r}
# table
knitr::kable(sum.results[,c(1:5)], booktabs = TRUE,
             caption = 'Summary of the experiment(part 1)')
knitr::kable(sum.results[,-c(1:5)], booktabs = TRUE,
             caption = 'Summary of the experiment(part 2)')

```



### Box plots of bias at all $t$
The following three boxplots show the bias under different situations. For each plot, we consider three $t$
values. And for each $t$ value, the black boxplot is the one for $n=100$, the gray one is for $n=1000$ and the white one is for $n=10000$. 


```{r}
# get the bias
bias <- array(dim = c(100, 3, 9))
truep <- t(matrix(rep(pnorm(t),3), nrow = 9, ncol = 3))
for (i in 1:100){
  bias[i,,] <- results[i,,]-truep
}

####### for t=0.0, 0.67, 0.84
prg1 <- vector()
for(a in 1:3){
  for (b in 1:3){
    prg1 <- c(prg1,sort(bias[,a,b])) 
  }
}

nprg <- 100
f1 <- rep(c(rep("0.0",nprg),rep("0.67",nprg),rep("0.84",nprg)),3) 
f2 <- c(rep(100,nprg*3),rep(1000,nprg*3),rep(10000,nprg*3))  

prgdata1 <- data.frame(b=factor(f1), 
                      Correlation=factor(f2),
                      PRG=prg1,geom="point")

#postscript(paste("1to3.eps",sep=""), width = 4, height = 4,horizontal=FALSE)
ggplot(aes(y = PRG, x = b, fill = Correlation), data = prgdata1) +
  geom_boxplot(notch=TRUE,notchwidth=0.3,outlier.size=2,outlier.shape=1) +
  scale_fill_manual(name = "Correlation", 
                    values = c("grey50", "grey75","white"))+
  ylab("Bias") + 
  xlab("Different t value")+
  theme_bw()+
  guides(fill=FALSE)+
  geom_hline(aes(yintercept=0), colour="black", linetype="dashed")


###### for t=1.28, 1.65, 2.32
prg2 <- vector()
for(a in 1:3){
  for (b in 4:6){
    prg2 <- c(prg2,sort(bias[,a,b])) 
  }
}

f3 <- rep(c(rep("1.28",nprg),rep("1.65",nprg),rep("2.32",nprg)),3) 
prgdata2 <- data.frame(b=factor(f3), 
                      Correlation=factor(f2),
                      PRG=prg2,geom="point")

#postscript(paste("4to5.eps",sep=""), width = 4, height = 4,horizontal=FALSE
ggplot(aes(y = PRG, x = b, fill = Correlation), data = prgdata2) +
  geom_boxplot(notch=TRUE,notchwidth=0.3,outlier.size=2,outlier.shape=1) +
  scale_fill_manual(name = "Correlation", 
                    values = c("grey50", "grey75","white"))+
  ylab("Bias") + 
  xlab("Different t value")+
  theme_bw()+
  guides(fill=FALSE)+
  geom_hline(aes(yintercept=0), colour="black", linetype="dashed")


##### for t=2.58, 3.09, 3.72
prg3 <- vector()
for(a in 1:3){
  for (b in 7:9){
    prg3 <- c(prg3,sort(bias[,a,b])) 
  }
}
f4 <- rep(c(rep("2.58",nprg),rep("3.09",nprg),rep("3.72",nprg)),3) 
prgdata3 <- data.frame(b=factor(f4), 
                      Correlation=factor(f2),
                      PRG=prg3,geom="point")

#postscript(paste("7to9.eps",sep=""), width = 4, height = 4,horizontal=FALSE)
ggplot(aes(y = PRG, x = b, fill = Correlation), data = prgdata3) +
  geom_boxplot(notch=FALSE,notchwidth=0.3,outlier.size=2,outlier.shape=1) +
  scale_fill_manual(name = "Correlation", 
                    values = c("grey50", "grey75","white"))+
  ylab("Bias") + 
  xlab("Different t value")+
  theme_bw()+
  guides(fill=FALSE)+
  geom_hline(aes(yintercept=0), colour="black", linetype="dashed")

```



